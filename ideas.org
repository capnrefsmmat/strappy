#+TITLE: Strappy ideas
#+AUTHOR: Alex Reinhart

* DONE Implement #lang strappy
  Provide racket/base plus a library of test statistics and such.

* DONE Write or provide a bunch of basic test statistics
  CLOSED: [2016-02-07 Sun 19:28]
  - mean
  - variance
  - std. dev.
  - median
  - min
  - max

* DONE Permutation tests
  CLOSED: [2016-02-07 Sun 19:28]
  Define macro or higher-order function which takes a function and returns a
  version which samples from permuted arguments.

* DONE Implement autodiff for all base functions so we can get transformed pdfs
  CLOSED: [2016-02-09 Tue 14:58]
  Try porting wmfarr's autodiff package to modern Racket, then apply it to all
  of our builtins. Then have function->statistic derive the pdf of the result
  from the pdfs of the arguments.

  https://planet.racket-lang.org/display.ss?package=deriv.plt&owner=wmfarr

** TODO Use numerical integration to get CDFs of transformed variables

* DONE Clarify distinction between define, define-statistic
  CLOSED: [2017-08-22 Tue 20:16]
  It's not currently obvious to readers which should be used when. Consider

  #+BEGIN_SRC racket
    (define-statistic (abs-mean-difference xs ys)
      (abs (- (mean xs) (mean ys))))

    (define permuted-mean-difference (permuted abs-mean-difference))
  #+END_SRC

  Why is =abs-mean-difference= a statistic and =permuted-mean-difference= not?

  One option is to make =define-statistic= accept a constant-defining form, so
  we can write
  #+BEGIN_SRC racket
    (define-statistic permuted-mean-difference (permuted abs-mean-difference))
  #+END_SRC

  That solves the problem in this case, but not in others. Maybe I should write
  more example code so the issue becomes clearer.

  Venture uses "assume" for defining random variables.

  We have a matrix of possible inputs/outputs:

  |             | Random out   | Constant out |
  |-------------+--------------+--------------|
  | Random in   | RV/statistic | n/a          |
  | Constant in | Distribution | Function     |

  Right now =define-statistic= is handling both statistics and functions, since it
  can produce random or constant outputs depending on its inputs. We use =define=
  for distributions and for constant values.

  Part of the problem is we don't really have "random variables": we have named
  distributions. =assuming= would give us random variables in scopes.

  Alternately: is =define-statistic= necessary if all base constructs provided to
  us are already statistics? Consider the =kde.rkt= example, where writing

  #+BEGIN_SRC racket
    (define-statistic (kde data bandwidth)
      (+ data (normal-dist 0 bandwidth)))
  #+END_SRC

  doesn't work (not sure why not), wrapping the =normal-dist= in =sample= works, but
  we can just write it as a =define= and it works fine. If all the pieces you
  compose ingest distributions and spit out distributions, do you need
  =define-statistic= at all?

  Nope -- every example written works without it!
* TODO What if a function uses the same argument in multiple places?

  Consider

  #+BEGIN_SRC racket
    (define (double x)
      (+ x x))
  #+END_SRC

  Here, =+= has been statistified, so it draws two samples from =x= and returns
  their sum.

  But what if I want /one/ sample from =x=, doubled?

  I guess this behavior makes sense if we exclusively talk about variables as
  distributions. If you want just one sample from them, use =sample=.

* TODO Distinguish between RVs and distributions
  Make a =define-distribution= for giving names to specific distributions.

  Then define an =assuming= macro that behaves like =let*= and creates random
  variables:
  
  #+BEGIN_SRC racket
    (assuming ([X ~ (normal-dist)]
               [eps ~ (normal-dist)]
               [Y ~ (+ (* 2 X) eps)])
              (do-stuff-with X Y))
  #+END_SRC

  Within the body of =assuming=, the variables are bound, and Y's value is
  correlated with X's as expected. The value of =assuming= is the distribution
  of the value of its body, over repeated draws from the bound random variables.

  Next, remove =define-statistic= entirely. We should just write functions of
  ordinary lists/vectors; inside the =assuming= body, random variables are just
  draws from their distributions.

  We still need functions that generate distributions (like one taking a sample
  size and returning the distribution of effect sizes), but these can just embed
  =assuming= blocks.

* TODO Write more examples
  Watch out for http://statcourse.com/misuse.htm
  - Contingency table permutation
    https://udel.edu/~mcdonald/statrandind.html
  - Precision calculation
    Choose a desired CI width, show proportion of CIs under that width for
    varying sample sizes
* TODO Custom printing of distribution objects
  https://docs.racket-lang.org/reference/Printer_Extension.html#%28def._%28%28lib._racket%2Fprivate%2Fbase..rkt%29._gen~3acustom-write%29%29

* TODO Error checking for my macros and functions
  Write contracts or error checks for each so they give reasonable errors when
  misused.
* TODO Is cross-validation possible to implement?
  We'd need some notion of "risk" and a sampler which draws from the folds of
  the data, then tests against the rest.

* TODO Borrow syntax restrictions and changes from the HtDP teaching languages
  https://docs.racket-lang.org/drracket/htdp-langs.html

  Case sensitivity would be a good one, for random variable notation
  conventions. Some of the other language simplifications would be good to
  prevent errors for new students.

* TODO Figure out how to handle ANOVA
* TODO Create mixture distribution constructor
  Provide a distribution and distributions on some of its arguments.

  Sampling is fairly easy, but is there a straightforward way to get the pdfs or
  cdfs?

* TODO Contingency table functions and chi-squared-like tests
* TODO Write an outline for /Statistics Done Right/
* TODO Clarify distinction between define, define-statistic
  It's not currently obvious to readers which should be used when. Consider

  #+BEGIN_SRC racket
    (define-statistic (abs-mean-difference xs ys)
      (abs (- (mean xs) (mean ys))))

    (define permuted-mean-difference (permuted abs-mean-difference))
  #+END_SRC

  Why is =abs-mean-difference= a statistic and =permuted-mean-difference= not?

  One option is to make =define-statistic= accept a constant-defining form, so
  we can write
  #+BEGIN_SRC racket
    (define-statistic permuted-mean-difference (permuted abs-mean-difference))
  #+END_SRC

  That solves the problem in this case, but not in others. Maybe I should write
  more example code so the issue becomes clearer.
* TODO Causal inference DSL
  It'd be nice to be able to postulate a set of causal relationships, then query
  the graph, asking, "If we condition on these nodes, can we infer the causal
  relationship between these other nodes?"

* TODO User-defined random distributions from their CDFs
  Using the autodiff features to get the PDF

  Something like this:

  #+BEGIN_SRC racket
    (define-distribution foo-dist
      ([shape 0]
       [scale 1])
      #:cdf (lambda (x) (some-function-of shape scale x))
      #:min-x 0
      #:max-x +inf.0
      #:pdf (lambda (x) (some-other-function)) ; optional
      #:inv-cdf (lambda (x) (inverse-of-cdf)) ; optional
      )
  #+END_SRC

  The pdf could be found by autodiff, the inv-cdf by numerical root-finding if
  absolutely necessary. (This is guaranteed to work because the cdf is monotone,
  but it's not guaranteed to be fast.)

  My macro skills are insufficient to implement this.

* TODO Find an elegant way of representing multivariate data
  If we want to be able to do regression, contingency tables, ANOVA, or anything
  multivariate, we need to represent data tables reasonably efficiently.

  One option is a list of structs, the struct defining the fields recorded for
  each observation. This gives the advantage of named fields. Columnwise
  operations, joins, groups, and so on wouldn't be very efficient, and there'd
  be no way to subset columns -- you get the whole struct or nothing.

* TODO Modeling language with automatically-generated MLEs
  Let the user define a likelihood function, then provide some data. Use
  autodiff to get the log-likelihood's maximum automatically with Newton-Raphson
  or something similar, plus CIs and so on.

  For simple models with iid data, we just need the contribution to the
  likelihood from single terms, and then we can sum up the logs and maximize.

  A sketch:

  #+BEGIN_SRC racket
    (define-likelihood ((regression data-row) intercept slope sigma)
      (normal-error (- (data-y data-row)
                       (+ intercept (* slope (data-x data-row))))
                    sigma))
  #+END_SRC
  where, importantly, =normal-error= is differentiable.

  Then we can do something like

  #+BEGIN_SRC racket
    (fit regression big-list-of-rows)
  #+END_SRC

  and get back a structure with MLEs, variance-covariance matrix, whatever.

  Open problems:
  - The range of error models we can use is pretty small. Least absolute
    deviations, for example, isn't differentiable
  - Emphasizes the error distribution, when in regression it's the least
    important assumption
  - What about, say, ANOVA, where we have discrete factor levels and we need to
    select which level is relevant to each row of data? A big =cond= or mess of
    =if= expressions would be rather ugly.
